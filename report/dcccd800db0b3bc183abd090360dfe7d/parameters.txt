clf__warm_start|False
clf__shuffle|False
clf__power_t|0.5
clf__verbose|0
tfidf__use_idf|True
clf__random_state|None
clf__n_jobs|1
vect__charset|None
vect__charset_error|None
clf__penalty|l2
clf__alpha|0.0001
vect__analyzer|word
clf__l1_ratio|0.15
tfidf__smooth_idf|True
tfidf__sublinear_tf|False
vect__max_df|0.5
clf__rho|None
clf|SGDClassifier(alpha=0.0001, class_weight=None, epsilon=0.1, eta0=0.0,
       fit_intercept=True, l1_ratio=0.15, learning_rate=optimal,
       loss=hinge, n_iter=5, n_jobs=1, penalty=l2, power_t=0.5,
       random_state=None, rho=None, shuffle=False, verbose=0,
       warm_start=False)
clf__n_iter|5
vect__lowercase|True
vect__binary|False
vect__stop_words|None
vect__encoding|utf-8
vect__max_features|5000
vect__dtype|<type 'numpy.int64'>
vect__strip_accents|None
clf__learning_rate|optimal
vect|CountVectorizer(analyzer=word, binary=False, charset=None, charset_error=None,
        decode_error=strict, dtype=<type 'numpy.int64'>, encoding=utf-8,
        input=content, lowercase=True, max_df=0.5, max_features=5000,
        min_df=1, ngram_range=(1, 2), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern=(?u)\b\w\w+\b, tokenizer=None,
        vocabulary=None)
clf__loss|hinge
vect__min_df|1
tfidf__norm|l2
vect__tokenizer|None
clf__eta0|0.0
vect__input|content
vect__ngram_range|(1, 2)
tfidf|TfidfTransformer(norm=l2, smooth_idf=True, sublinear_tf=False, use_idf=True)
vect__preprocessor|None
vect__vocabulary|None
clf__fit_intercept|True
vect__decode_error|strict
clf__epsilon|0.1
clf__class_weight|None
vect__token_pattern|(?u)\b\w\w+\b
position_relevance_lt3_gt3_e3
length train	23164
length test	9928